---
title: "Planning GPT for Social Research: How and Whether Large Language Models Can Help Social Scientists"
author: "Musashi Jacobs-Harukawa"
---

# Rough Planning



- _Context:_ You're a social scientist that's hearing a lot about LLMs and how big a deal they are.
- In this lecture, I want to take a step back and give a relatively hype-free explanation of:
	- What are LLMs/foundation models
		- Mechanically what do they do
		- How are they trained (and what does training mean?)
	- What is completion, zero-shot learning, in-context learning, few-shot learning



## General Structure



# Reading Notes

Relevant papers:


- Using LLMs to learn about the world:
	- Argyle et al 2023
	- Wu et al 2023
	- Also discuss precedence of neural LMs as tool for corpus summary/description, e.g. Rodman etc.

- Using LLMs for zero-shot classification
	- Ornstein et al 2022
	- Gilardi et al 2023

- Ethical Aspects of LLMs
	- Bender et al 2021 (?)

- Technical Papers on LLMs
	- Vaswani et al?
	- Bradford GPT-2
	- Foundation Models paper (Bommasani?)

# Drafting


