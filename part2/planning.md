---
title: "Planning GPT for Social Research: How and Whether Large Language Models Can Help Social Scientists"
author: "Musashi Jacobs-Harukawa"
---

# Rough Planning

- _Context:_ You're a social scientist that's hearing a lot about LLMs and how big a deal they are.
- In this lecture, I want to take a step back and give a relatively hype-free explanation of:
	- What are LLMs/foundation models
		- Mechanically what do they do
		- How are they trained (and what does training mean?)
	- What is completion, zero-shot learning, in-context learning, few-shot learning



## Rough Intro

- What's been going on in the world of AI?
- Here are my observations about how it's already impacting social science research.
	- Give some examples
- I think some applications are better than others (valid inference, less problematic)
- and I think that some of the worse applications are based in part on a misconception of what this technology is.
- So, I want to first do a deep-ish dive on what LLMs are and how they work so we can be more specific about what inferences we can make from them.
- Then I'll come back to some key pieces of research that are coming out in this area and discuss them within the context of the explanation I have just given.


- Context/Format - catching you up on what is happening, what it means, and how it relates to us
- What people are already doing
	- One model to rule them all?
	- Using LLMs to study society?
- How it works (in a relevant level of detail?)
	- Language as conditional probabilities / autoregressive language generation
	- Training a generative model
	- Completion -> everything else
		- Zero-shot and in-context learning
		- Prompt engineering
	- Instruction-tuning
	- What's coming next?...
		- Bigger
		- Smaller
- Considerations
	- Reproducibility
	- Transparency
	- Privacy
- Discussing papers in more detail?



## How it Works


# Reading Notes

Relevant papers:


- Using LLMs to learn about the world:
	- Argyle et al 2023
	- Wu et al 2023
	- OpinionQA
	- Also discuss precedence of neural LMs as tool for corpus summary/description, e.g. Rodman etc.


- Using LLMs for zero-shot classification
	- Ornstein et al 2022
	- Gilardi et al 2023

- Ethical Aspects of LLMs
	- Bender et al 2021 (?)

- Technical Papers on LLMs
	- Vaswani et al?
	- Bradford GPT-2
	- Foundation Models paper (Bommasani?)

# Drafting


