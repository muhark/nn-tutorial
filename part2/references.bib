@comment{x-kbibtex-encoding=utf-8}

@article{wu2023,
	author = {Wu, Patrick Y and Tucker, Joshua A and Nagler, Jonathan and Messing, Solomon},
	journal = {arXiv preprint arXiv:2303.12057},
	title = {{Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting}},
	year = {2023}
}

@article{argyle2023,
	author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
	doi = {10.1017/pan.2023.2},
	journal = {Political Analysis},
	pages = {1–15},
	publisher = {Cambridge University Press},
	title = {{Out of One, Many: Using Language Models to Simulate Human Samples}},
	year = {2023}
}

@article{ornstein2022,
	author = {Ornstein, Joseph T and Blasingame, Elise N and Truscott, Jake S},
	title = {{How to Train Your Stochastic Parrot: Large Language Models for Political Texts}},
	year = {2022}
}

@article{gilardi2023,
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	journal = {arXiv preprint arXiv:2303.15056},
	title = {{ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks}},
	year = {2023}
}

@inproceedings{brown2020,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {{Advances in Neural Information Processing Systems}},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
	pages = {1877–1901},
	publisher = {Curran Associates, Inc.},
	title = {{Language Models are Few-Shot Learners}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	volume = {33},
	year = {2020}
}

@article{bommasani2021,
	author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
	journal = {arXiv preprint arXiv:2108.07258},
	title = {{On the opportunities and risks of foundation models}},
	year = {2021}
}

@techreport{palmer2023,
	author = {Palmer, Alexis and Spirling, Arthur},
	institution = {Working paper},
	title = {{Large Language Models Can Argue in Convincing and Novel Ways About Politics: Evidence from Experiments and Human Judgement}},
	year = {2023}
}

@misc{santurkar2023,
	archiveprefix = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	eprint = {2303.17548},
	primaryclass = {cs.CL},
	title = {{Whose Opinions Do Language Models Reflect?}},
	year = {2023}
}

@misc{wu2023bloomberg,
	archiveprefix = {arXiv},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	eprint = {2303.17564},
	primaryclass = {cs.LG},
	title = {{BloombergGPT: A Large Language Model for Finance}},
	year = {2023}
}

@misc{li2022,
	archiveprefix = {arXiv},
	author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
	eprint = {2208.03306},
	primaryclass = {cs.CL},
	title = {{Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}},
	year = {2022}
}

@misc{gururangan2023,
	archiveprefix = {arXiv},
	author = {Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
	eprint = {2303.14177},
	primaryclass = {cs.CL},
	title = {{Scaling Expert Language Models with Unsupervised Domain Discovery}},
	year = {2023}
}

@article{kocon2023,
	author = {Kocoń, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydło, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and others},
	journal = {arXiv preprint arXiv:2302.10724},
	title = {{ChatGPT: Jack of all trades, master of none}},
	year = {2023}
}

@misc{ouyang2022,
	archiveprefix = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	eprint = {2203.02155},
	primaryclass = {cs.CL},
	title = {{Training language models to follow instructions with human feedback}},
	year = {2022}
}

