<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Dr Musashi Jacobs-Harukawa, DDSS Princeton">
  <meta name="dcterms.date" content="2023-03-23">
  <title>Demystifying Deep Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/white.css" id="theme">
  <link rel="stylesheet" href="minimal-theme.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Demystifying Deep Learning</h1>
  <p class="subtitle">An Introduction to Neural Networks for Social
Scientists</p>
  <p class="author">Dr Musashi Jacobs-Harukawa, DDSS Princeton</p>
  <p class="date">23 Mar 2023</p>
</section>

<section>
<section id="motivation" class="title-slide slide level1">
<h1>Motivation</h1>

</section>
<section id="who-is-this-for" class="slide level2">
<h2>Who is this for?</h2>
<ul>
<li class="fragment">Many social scientists use quantitative approaches
to study complex questions.</li>
<li class="fragment">Familiarity with statistics, maybe also machine
learning.</li>
<li class="fragment">But <strong>deep learning</strong> seems
daunting.</li>
<li class="fragment">Aim is not to simplify subject, but to translate
existing knowledge and expertise.</li>
</ul>
<aside class="notes">
<ul>
<li>In short, me from a few years ago.</li>
<li>Received training in quantitative methods, causal inference, some
machine learning</li>
<li>But deep learning always treated as a separate super-complicated
thing</li>
<li>Lots of resources for learning deep learning out there, but focus
tends to be for (I assume) a CS audience.</li>
<li>As quantitative social scientists, we’re not seeking to avoid the
math–on the contrary, we want to understand the tools and the resulting
models. More on this in a bit.</li>
</ul>
</aside>
</section>
<section id="what-is-deep-learning" class="slide level2">
<h2>What is Deep Learning?</h2>
<ul>
<li class="fragment">Approach to using computers to learn patterns in
data</li>
<li class="fragment">based around a modular set of tools loosely
inspired by biological neurons.
<!--- Basis of many advanced systems seen today.--></li>
</ul>
<aside class="notes">
<ul>
<li>“Machine learning” (it is in fact arguably a subfield)</li>
</ul>
</aside>
</section>
<section id="why-learn-about-deep-learning" class="slide level2">
<h2>Why learn about Deep Learning?</h2>
<ul>
<li class="fragment"><strong>Opportunity</strong>: <em>incredibly</em>
powerful tool for modelling complex processes
<ul>
<li class="fragment">Image processing (Computer Vision)</li>
<li class="fragment">Language (Natural Language Processing)</li>
</ul></li>
<li class="fragment"><strong>Engagement</strong>: increasingly applied
in social sciences
<ul>
<li class="fragment">Extracting visual features <a
href="https://www.cambridge.org/core/journals/political-analysis/article/learning-to-see-convolutional-neural-networks-for-the-analysis-of-social-science-data/7417AE7F021C92CA32DC04D4D4282B90">(Torres
and Cantú 2021)</a>; Multiple imputation <a
href="https://www.cambridge.org/core/journals/political-analysis/article/midas-touch-accurate-and-scalable-missingdata-imputation-with-deep-learning/5007854F57E88AF16D69BCCA4C5AF1FF">(Lall
and Robinson 2021)</a>; Multilingual embeddings <a
href="https://www.cambridge.org/core/journals/political-analysis/article/crosslingual-classification-of-political-texts-using-multilingual-sentence-embeddings/30689C8798F097EEBA514ABE4891A71B">(Licht
2023)</a>; Automated coding of videos <a
href="https://www.cambridge.org/core/journals/political-analysis/article/automated-coding-of-political-campaign-advertisement-videos-an-empirical-validation-study/7B60C86AAC9E71016F9397D2FD247F8C">(Tarr,
Hwang and Imai 2023)</a></li>
</ul></li>
<li class="fragment"><strong>Relevance</strong>: Increasingly directly
affecting society
<ul>
<li class="fragment"><a
href="https://arxiv.org/pdf/2303.10130.pdf">Potential Labor Market
Impact of LLMs</a></li>
</ul></li>
</ul>
<aside class="notes">
<ul>
<li>1st point is about research opportunities</li>
<li>2nd point is about need to engage with existing research</li>
<li>3rd point is about object of research</li>
</ul>
</aside>
</section>
<section id="why-learn-the-fundamentals" class="slide level2">
<h2>Why learn the fundamentals?</h2>
<ul>
<li class="fragment">Can get by with minimal understanding of “how” it
works.</li>
<li class="fragment">But we’re also interested in characterizing scope
of claims, predictions.</li>
<li class="fragment">DL is about scaling; fundamentals scale.</li>
</ul>
</section>
<section id="learning-objectives" class="slide level2">
<h2>Learning Objectives</h2>
<ul>
<li class="fragment"><p>The basic building block of neural networks:
artificial neurons.</p></li>
<li class="fragment"><p>How to learn from data with
<strong>optimization</strong>.</p></li>
<li class="fragment"><p>How and why “ensembling” neurons is
powerful.</p></li>
<li class="fragment"><p><em>An intuition for an “engineer’s approach” to
modelling data.</em></p></li>
</ul>
</section>
<section id="tldr-lecture-in-one-slide" class="slide level2">
<h2>tl;dr (Lecture in One Slide)</h2>
<ol type="1">
<li class="fragment"><strong>Neural networks</strong> are basically
nested regression models with discontinuities.</li>
<li class="fragment"><strong>Loss gradients</strong> link model
parameters to the overall accuracy of the model.</li>
<li class="fragment"><strong>Backpropagation</strong> is a technique
that allows us to calculate the loss gradient of every parameter in a
network.</li>
<li class="fragment"><strong>Gradient descent</strong> is an algorithm
for updating parameters using loss gradients to improve accuracy.</li>
<li class="fragment">By “stacking” neurons and introducing
discontinuities, we can learn complex patterns in data.</li>
</ol>
</section></section>
<section>
<section id="simple-case-one-x-one-y" class="title-slide slide level1">
<h1>Simple Case: One X, One Y</h1>
<p><em>We learn how to “manually” fit linear regressions.</em></p>
</section>
<section id="problem" class="slide level2">
<h2>Problem</h2>
<ul>
<li class="fragment">We observe two processes, <span
class="math inline">\(y\)</span> and <span
class="math inline">\(x\)</span>.</li>
<li class="fragment">We want to describe the <em>average linear</em>
relationship between them.</li>
<li class="fragment">Model: <span class="math inline">\(y = \beta_1x +
\beta_0\)</span></li>
<li class="fragment">Parameters: <span
class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\beta_0\)</span> ← find optimal values for
these</li>
<li class="fragment">In their language:
<ul>
<li class="fragment"><span class="math inline">\(\beta_1\)</span> is
<span class="math inline">\(w\)</span> (weight)</li>
<li class="fragment"><span class="math inline">\(\beta_0\)</span> is
<span class="math inline">\(b\)</span> (bias).</li>
</ul></li>
</ul>
</section>
<section id="data-and-ols-solution" class="slide level2">
<h2>Data (and OLS Solution)</h2>
<iframe width="100%" height="576px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure1.html">
</iframe>
</section>
<section id="a-general-procedure-for-fitting-models"
class="slide level2">
<h2>A General Procedure for Fitting Models</h2>
<ol start="0" type="1">
<li class="fragment">Begin with a random guess of the correct
model.</li>
<li class="fragment">Generate prediction using model and data.</li>
<li class="fragment">Measure error.</li>
<li class="fragment">Use this information to adjust the model.</li>
</ol>
</section>
<section id="a-random-guess" class="slide level2">
<h2>A Random Guess</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure2.html">
</iframe>
<ul>
<li class="fragment"><span class="math inline">\(w_{init} =
0.77\)</span>, <span class="math inline">\(b_{init} =
-0.15\)</span></li>
</ul>
</section>
<section id="starting-with-a-single-data-point" class="slide level2">
<h2>Starting with a Single Data Point</h2>
<ul>
<li class="fragment">We can do this procedure with one, a few, or all
data points.</li>
<li class="fragment">The difference is non-trivial, but the procedure is
the same.</li>
<li class="fragment">Begin with single point for simplicity.
<ul>
<li class="fragment"><span class="math inline">\((x, y) = (1.37,
3.91)\)</span></li>
</ul></li>
</ul>
</section>
<section id="generate-prediction" class="slide level2">
<h2>Generate Prediction</h2>
<ul>
<li class="fragment">Generate prediction:
<ul>
<li class="fragment"><span class="math inline">\(\hat{y} = wx +
b\)</span></li>
<li class="fragment"><span class="math inline">\(x=1.37\)</span></li>
<li class="fragment"><span class="math inline">\(0.91_{\hat{y}} \approx
(0.77)_w\times(1.37)_x + (-0.15)_b\)</span></li>
</ul></li>
<li class="fragment">Called <strong>Forward Pass</strong></li>
</ul>
</section>
<section id="computing-loss" class="slide level2">
<h2>Computing Loss</h2>
<ul>
<li class="fragment">True <span class="math inline">\(y\)</span> is
<span class="math inline">\(3.91\)</span>.
<ul>
<li class="fragment">Error: <span
class="math inline">\(y-\hat{y}=3.00\)</span></li>
</ul></li>
<li class="fragment">Loss function: <span class="math inline">\(L(w,
b)\)</span>
<ul>
<li class="fragment">Expresses loss as function of <span
class="math inline">\(w\)</span> and <span
class="math inline">\(b\)</span></li>
<li class="fragment"><em>Squared Error</em>: <span
class="math inline">\(L(w, b) = (y - \hat{y})^2\)</span></li>
</ul></li>
<li class="fragment">We need to square or take the absolute value.</li>
<li class="fragment">Why? Will become clear.</li>
</ul>
<aside class="notes">
<ul>
<li>The operations we have are minimization or maximization of a
score</li>
<li>If we take error, then the minimum error is at negative
infinity</li>
<li>We want error to be <em>zero</em>, which is where loss is
<em>minimized</em>.</li>
</ul>
</aside>
</section>
<section id="how-can-we-use-this-information" class="slide level2">
<h2>How can we use this information?</h2>
<ul>
<li class="fragment"><em>Aim</em>: find values of <span
class="math inline">\(w\)</span> and <span
class="math inline">\(b\)</span> that make model as correct as possible.
<ul>
<li class="fragment">Alternatively: find <span class="math inline">\(w,
b\)</span> that minimize our error.</li>
</ul></li>
<li class="fragment">We can use partial derivatives!
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta L(w,
b)}{\delta w}\)</span>: describes how <span
class="math inline">\(L\)</span> changes as we adjust <span
class="math inline">\(w\)</span>.</li>
<li class="fragment"><span class="math inline">\(\frac{\delta L(w,
b)}{\delta b}\)</span>: describes how <span
class="math inline">\(L\)</span> changes as we adjust <span
class="math inline">\(b\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="understanding-gradients" class="slide level2">
<h2>Understanding Gradients</h2>
<ul>
<li class="fragment">I find calculating it “by hand” helps me understand
what’s happening.</li>
<li class="fragment"><a
href="https://en.wikipedia.org/wiki/Partial_derivative#Definition">Definition
of a partial derivative</a>:</li>
</ul>
<div class="fragment">
<p><span class="math display">\[
\frac{\delta f(x)}{\delta x} \;=\; \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}
\]</span></p>
</div>
</section>
<section id="numerical-calculation" class="slide level2">
<h2>Numerical calculation</h2>
<p><span class="math display">\[
\frac{\delta f(x)}{\delta x} \;=\; \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}
\]</span></p>
<div class="fragment">
<div class="sourceCode" id="cb1"><pre
class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Actual Python/PyTorch Code</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>h <span class="op">=</span> <span class="fl">0.0001</span>                           <span class="co"># h as some arbitrarily small value</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>fx <span class="op">=</span>  (y0 <span class="op">-</span> (x0 <span class="op">*</span>  w    <span class="op">+</span> b)).<span class="bu">pow</span>(<span class="dv">2</span>) <span class="co"># f(x)   (squared loss)</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>fxh <span class="op">=</span> (y0 <span class="op">-</span> (x0 <span class="op">*</span> (w<span class="op">+</span>h) <span class="op">+</span> b)).<span class="bu">pow</span>(<span class="dv">2</span>) <span class="co"># f(x+h)</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>dfx <span class="op">=</span> (fxh<span class="op">-</span>fx)<span class="op">/</span>h                     <span class="co"># (f(x+h)-f(x))/h</span></span></code></pre></div>
<ul>
<li class="fragment">When <span class="math inline">\(x, y = 1.37,
3.91\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta L}{\delta
w} \approx -8.23\)</span></li>
</ul>
</div>
</section>
<section id="how-do-we-use-this-gradient" class="slide level2">
<h2>How do we use this gradient?</h2>
<p><span class="math inline">\(-8.23\)</span> tells us:</p>
<ul>
<li class="fragment">when <span
class="math inline">\(x=1.37\)</span></li>
<li class="fragment">if we increase <span
class="math inline">\(w\)</span> by <span
class="math inline">\(1\)</span></li>
<li class="fragment">our loss will <em>decrease</em> at a rate of <span
class="math inline">\(8.23\)</span></li>
<li class="fragment">So we should increase <span
class="math inline">\(w\)</span>.</li>
</ul>
</section>
<section id="visualizing-the-gradient" class="slide level2">
<h2>Visualizing the Gradient</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure3.html">
</iframe>
<aside class="notes">
<ul>
<li>Look at how loss changes as we vary <span
class="math inline">\(w\)</span> first.</li>
<li>Then show how that is reflected in the gradient.</li>
<li>Make point about non-unique solution for two parameters with single
data point</li>
</ul>
</aside>
</section>
<section id="how-much-to-adjust-w" class="slide level2">
<h2>How much to adjust <span class="math inline">\(w\)</span>?</h2>
<ul>
<li class="fragment">Why not change <span
class="math inline">\(w\)</span> to minimize loss for each
observation?</li>
</ul>
</section>
<section id="gradient-descent" class="slide level2">
<h2>Gradient Descent</h2>
<p><span class="math display">\[
w&#39; = w - \eta \frac{\delta L(w, b)}{\delta w}
\]</span></p>
<ul>
<li class="fragment"><span class="math inline">\(w\)</span> is the
current value of the parameter</li>
<li class="fragment"><span class="math inline">\(\frac{\delta L}{\delta
w}\)</span> is the partial derivative of the loss function wrt the
parameter.</li>
<li class="fragment"><span class="math inline">\(w&#39;\)</span> is the
new value of the parameter</li>
<li class="fragment"><span class="math inline">\(\eta\)</span> is the
<em>Learning Rate</em>.</li>
</ul>
</section>
<section id="how-large-is-eta" class="slide level2">
<h2>How large is <span class="math inline">\(\eta\)</span>?</h2>
<ul>
<li class="fragment">Usually some small value (<span
class="math inline">\(0.05\)</span> or smaller).</li>
<li class="fragment">Hyperparameter and its optimal value is an
empirical question.</li>
<li class="fragment">Meme value is <a
href="https://twitter.com/karpathy/status/801621764144971776?lang=en"><span
class="math inline">\(3e-4\)</span></a></li>
</ul>
<div class="fragment">
<p><img data-src="figures/3e-4.png" /></p>
</div>
</section>
<section id="our-new-model" class="slide level2">
<h2>Our New Model</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure4.html">
</iframe>
</section>
<section id="repeat-for-full-dataset-one-epoch" class="slide level2">
<h2>Repeat for Full Dataset (One Epoch)</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure5.html">
</iframe>
</section>
<section id="stochastic-batch-mini-batch" class="slide level2">
<h2>Stochastic, Batch, Mini-Batch</h2>
<ul>
<li class="fragment">Stochastic: one observation at a time</li>
<li class="fragment">Batch: whole dataset at once</li>
<li class="fragment">Mini-batch: smaller-than-dataset batches</li>
<li class="fragment">For batch and mini-batch, we need a different loss
function
<ul>
<li class="fragment"><em>Mean Squared Loss</em>: <span
class="math inline">\(\frac{1}{N}\sum_i(y_i - \hat{y})^2\)</span></li>
</ul></li>
</ul>
</section>
<section id="what-did-we-just-learn" class="slide level2">
<h2>What did we just learn?</h2>
<ul>
<li class="fragment"><del>A slower and less precise method for linear
regression.</del></li>
<li class="fragment">We can use the partial derivative of a loss
function…
<ul>
<li class="fragment"><em>the direction of decreasing loss</em></li>
</ul></li>
<li class="fragment">… to adjust the parameters of a model…
<ul>
<li class="fragment"><em>gradient descent updates</em></li>
</ul></li>
<li class="fragment">… to fit it to our data.</li>
</ul>
</section></section>
<section>
<section id="two-x-binary-y" class="title-slide slide level1">
<h1>Two X, Binary Y</h1>
<p><em>Where linear regression won’t cut it</em>.</p>
</section>
<section id="challenging-shape" class="slide level2">
<h2>Challenging Shape</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure6.html">
</iframe>
</section>
<section id="two-inputs" class="slide level2">
<h2>Two Inputs</h2>
<p><img data-src="figures/nn_2_inputs.png" /></p>
<div class="fragment">
<p>is the same as</p>
</div>
<div class="fragment">
<p><span class="math inline">\(Y = \beta_2X_2 + \beta_1X_1 +
\beta_0\)</span></p>
</div>
</section>
<section id="random-initialization" class="slide level2">
<h2>Random Initialization</h2>
<iframe width="110%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure7.html">
</iframe>
</section>
<section id="model-after-training" class="slide level2">
<h2>Model After Training</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure8.html">
</iframe>
</section>
<section id="doing-better" class="slide level2">
<h2>Doing Better</h2>
<p>For this data, we want a model that can:</p>
<ul>
<li class="fragment">Fit a more complicated response surface (than a
straight line)</li>
<li class="fragment">Do classification instead of regression</li>
</ul>
</section>
<section id="what-if-we-started-layering-our-models"
class="slide level2">
<h2>What if we started layering our models?</h2>
<p><img data-src="figures/nn_2layer_1input.png" /></p>
<ul>
<li class="fragment"><span class="math inline">\(h_1 = w_{1}x +
b_{1}\)</span></li>
<li class="fragment"><span class="math inline">\(y = w_{2}h_1 +
b_{2}\)</span></li>
<li class="fragment"><span class="math inline">\(y = w_{2}(w_{1}x +
b_{1}) + b_{2}\)</span></li>
</ul>
</section>
<section id="and-stacking-it" class="slide level2">
<h2>And stacking it?</h2>
<p><img data-src="figures/nn_141.png" /></p>
<ul>
<li class="fragment"><span class="math inline">\(h_i = w_{1i}x +
b_{1i}\)</span> (bias omitted from diagram for simplicity)</li>
<li class="fragment"><span class="math inline">\(y = \sum_i w_{2i}h_i +
b_{2i}\)</span></li>
<li class="fragment"><span class="math inline">\(y = \sum_i
w_{2i}(w_{1i}x + b_{1i}) + b_{2i}\)</span></li>
</ul>
</section>
<section id="we-could-create-a-model-like-this" class="slide level2">
<h2>We could create a model like this:</h2>
<p><img data-src="figures/nn_241.png" /></p>
</section>
<section id="but-how-do-we-train-this-model" class="slide level2">
<h2>But how do we train this model?</h2>
<p><img data-src="figures/nn_2layer_1input.png" /></p>
<ul>
<li class="fragment">If we can calculate loss gradients, we can use
Gradient Descent.</li>
<li class="fragment">Define loss wrt parameters: <span
class="math inline">\(L(w_1, b_1, w_2, b_2)\)</span></li>
<li class="fragment">How do we calculate <span
class="math inline">\(\frac{\delta L}{\delta w_1}\)</span>, <span
class="math inline">\(\frac{\delta L}{\delta w_2}\)</span>, etc.?
<ul>
<li class="fragment">We could use the numerical method before.</li>
<li class="fragment">But this requires 2 forward passes <em>per
parameter</em>.</li>
<li class="fragment">There’s something much faster.</li>
</ul></li>
</ul>
</section>
<section id="backpropagation" class="slide level2">
<h2>Backpropagation</h2>
<ul>
<li class="fragment">Using the chain rule, we can state all gradients as
a product of gradients on this graph.</li>
<li class="fragment">From this we can derive expressions for all
gradients as functions of known parameters, initial input and final
output.</li>
<li class="fragment">DL libraries manage all of this under the
hood.</li>
</ul>
</section>
<section id="breaking-down-lw_1-b_1-w_2-b_2" class="slide level2">
<h2>Breaking down <span class="math inline">\(L(w_1, b_1, w_2,
b_2)\)</span></h2>
<p><img data-src="figures/backprop.png" /></p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li class="fragment"><span class="math inline">\(L = (y -
\hat{y})^2\)</span>
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta L}{\delta
\hat{y}} = 2(y-\hat{y})\)</span></li>
</ul></li>
<li class="fragment"><span class="math inline">\(\hat{y} = w_{2}h_1 +
b_{2}\)</span>
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta
\hat{y}}{\delta w_2} = h_1\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
\hat{y}}{\delta h_1} = w_2\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
\hat{y}}{\delta b_2} = 1\)</span></li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li class="fragment"><span class="math inline">\(h_1 = w_{1}x +
b_{1}\)</span>
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta
h_1}{\delta w_1} = x\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
h_1}{\delta b_1} = 1\)</span></li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="gradient-for-w_1" class="slide level2">
<h2>Gradient for <span class="math inline">\(w_1\)</span></h2>
<p><span class="math inline">\(\frac{\delta L}{\delta \hat{w_2}} =
?\)</span></p>
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta L}{\delta
\hat{y}} = 2(y-\hat{y})\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
\hat{y}}{\delta w_2} = h_1\)</span></li>
</ul>
<div class="fragment">
<p>Applying the chain rule:</p>
<p><span class="math display">\[\begin{align}
\frac{\delta L}{\delta w_1} &amp;= \frac{\delta L}{\delta\hat{y}}
\frac{\delta \hat{y}}{\delta w_2} \\
                            &amp;= 2(y-\hat{y}) h_1 \\
                            &amp;= 2(y-(w_{2}(w_{1}x + b_{1}) + b_{2}))
w_{1}x + b_{1}

\end{align}\]</span></p>
</div>
</section>
<section id="gradient-for-w_2" class="slide level2">
<h2>Gradient for <span class="math inline">\(w_2\)</span></h2>
<p><span class="math inline">\(\frac{\delta L}{\delta \hat{w_1}} =
?\)</span></p>
<ul>
<li class="fragment"><span class="math inline">\(\frac{\delta L}{\delta
\hat{y}} = 2(y-\hat{y})\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
\hat{y}}{\delta h_1} = w_2\)</span></li>
<li class="fragment"><span class="math inline">\(\frac{\delta
h_1}{\delta w_1} = x\)</span></li>
</ul>
<div class="fragment">
<p>Applying the chain rule:</p>
<p><span class="math display">\[\begin{align}
\frac{\delta L}{\delta w_1} &amp;= \frac{\delta L}{\delta\hat{y}}
\frac{\delta\hat{y}}{\delta h_1} \frac{\delta h_1}{\delta w_1} \\
                            &amp;= 2(y-\hat{y}) w_2 x \\
                            &amp;= 2(y-(w_{2}(w_{1}x + b_{1})) + b_{2})
w_{2}x

\end{align}\]</span></p>
</div>
</section>
<section id="theory-in-code" class="slide level2">
<h2>Theory in Code</h2>
<p><img data-src="figures/nn_241.png" /></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Define Model</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>layer1  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">2</span>, features_out<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-3"><a href="#cb2-3"></a>layer2  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">4</span>, features_out<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>model   <span class="op">=</span> torch.nn.Sequential(layer1, layer2)</span></code></pre></div>
</section>
<section id="mini-batch-gradient-descent-with-mse-loss"
class="slide level2">
<h2>Mini-Batch Gradient Descent with MSE Loss</h2>
<div class="sourceCode" id="cb3"><pre
class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Define Model</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>layer1  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">2</span>, features_out<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb3-3"><a href="#cb3-3"></a>layer2  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">4</span>, features_out<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4"></a>model   <span class="op">=</span> torch.nn.Sequential(layer1, layer2)</span>
<span id="cb3-5"><a href="#cb3-5"></a></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># Define Batch Sampler, Loss Function and Optimization</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>data    <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">8</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-8"><a href="#cb3-8"></a>loss_fn <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb3-9"><a href="#cb3-9"></a>optim   <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">5e-4</span>)</span></code></pre></div>
</section>
<section id="training-a-model-for-50-epochs" class="slide level2">
<h2>Training a Model for 50 Epochs</h2>
<div class="sourceCode" id="cb4"><pre
class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Define Model</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>layer1  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">2</span>, features_out<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a>layer2  <span class="op">=</span> SimpleLinearNN(features_in<span class="op">=</span><span class="dv">4</span>, features_out<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-4"><a href="#cb4-4"></a>model   <span class="op">=</span> torch.nn.Sequential(layer1, layer2)</span>
<span id="cb4-5"><a href="#cb4-5"></a></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co"># Define Batch Sampler, Loss Function and Optimization</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>data    <span class="op">=</span> DataLoader(dataset, batch_size<span class="op">=</span><span class="dv">8</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-8"><a href="#cb4-8"></a>loss_fn <span class="op">=</span> torch.nn.MSELoss()</span>
<span id="cb4-9"><a href="#cb4-9"></a>optim   <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># Training</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb4-13"><a href="#cb4-13"></a>    <span class="cf">for</span> inputs, labels <span class="kw">in</span> data: <span class="co"># X, y</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>        optim.zero_grad() <span class="co"># Reset gradients</span></span>
<span id="cb4-15"><a href="#cb4-15"></a>        preds <span class="op">=</span> model.forward(inputs) <span class="co"># Forward pass</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>        loss <span class="op">=</span> loss_fn(preds, labels) <span class="co"># Loss calc</span></span>
<span id="cb4-17"><a href="#cb4-17"></a>        loss.backward()   <span class="co"># Calculate gradients</span></span>
<span id="cb4-18"><a href="#cb4-18"></a>        optim.step()      <span class="co"># Update parameters</span></span></code></pre></div>
</section>
<section id="but-does-it-do-better" class="slide level2">
<h2>But: does it do better?</h2>
<div class="fragment">
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure9.html">
</iframe>
<ul>
<li class="fragment">Turns out: stacking/layering linear models reduces
to a linear model!</li>
</ul>
</div>
</section></section>
<section>
<section id="deep-learning-basics" class="title-slide slide level1">
<h1>Deep Learning Basics</h1>
<p><em>Now we’re doing deep learning!</em></p>
</section>
<section id="the-missing-ingredient" class="slide level2">
<h2>The Missing Ingredient</h2>
<ul>
<li class="fragment">Final missing ingredient is <em>non-linear
activation functions</em>.</li>
<li class="fragment">You already know an example: logistic
regression!</li>
</ul>
</section>
<section id="logistic-regression" class="slide level2">
<h2>Logistic regression?</h2>
<p>Two components:</p>
<ol type="1">
<li class="fragment">Linear model: <span class="math inline">\(z =
\beta_1x + \beta_0\)</span></li>
<li class="fragment">Sigmoid transformation: <span
class="math inline">\(y = \frac{1}{1+e^{-z}}\)</span></li>
</ol>
<div class="fragment">
<iframe width="100%" height="300px" frameborder="0" seamless="seamless" scrolling="no" src="figures/sigmoid.html">
</iframe>
<ul>
<li class="fragment"><span class="math inline">\(\sigma(x)\)</span>
bounded between 0 and 1</li>
<li class="fragment">When <span class="math inline">\(x=0\)</span>,
<span class="math inline">\(\sigma(x)=0.5\)</span>.</li>
</ul>
</div>
</section>
<section id="artificial-neuron" class="slide level2">
<h2>Artificial Neuron</h2>
<ul>
<li class="fragment">Logistic regression is an example of an artificial
neuron, the basic building block of neural networks.</li>
<li class="fragment">Generally: activation function <span
class="math inline">\(\phi(\cdot)\)</span> applied to weighted sum of
inputs <span class="math inline">\(\sum_i w_ix_i\)</span>:</li>
</ul>
<div class="fragment">
<p><span class="math display">\[
    y = \phi(\sum_i w_ix_i)
\]</span></p>
</div>
</section>
<section id="neuron-with-sigmoid-activation" class="slide level2">
<h2>Neuron with Sigmoid Activation</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure10.html">
</iframe>
<ul>
<li class="fragment">Flat decision boundary</li>
<li class="fragment">Predictions bounded between 0 and 1</li>
</ul>
</section>
<section id="deep-neural-network-2-4-1" class="slide level2">
<h2>Deep Neural Network: 2-4-1</h2>
<div class="fragment">
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure11.html">
</iframe>
</div>
</section>
<section id="wider-and-deeper-neural-network-2-8-8-1"
class="slide level2">
<h2>Wider and Deeper Neural Network: 2-8-8-1</h2>
<div class="fragment">
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure12.html">
</iframe>
</div>
</section>
<section id="when-to-stop-training" class="slide level2">
<h2>When to stop training?</h2>
<p>Difficult question. A plausible strategy is:</p>
<ul>
<li class="fragment">Leave some training data out for
<em>evaluation</em>.</li>
<li class="fragment">Stop training when loss on data passes some
threshold.</li>
</ul>
</section>
<section id="network-with-eval-loss-0.52" class="slide level2">
<h2>2-4-1 Network with Eval Loss &lt;= 0.52</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure13.html">
</iframe>
</section>
<section id="more-complex-networks-generally-converge-faster"
class="slide level2">
<h2>More Complex Networks Generally Converge Faster</h2>
<iframe width="100%" height="500px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure14.html">
</iframe>
</section>
<section id="other-activation-functions" class="slide level2">
<h2>Other Activation Functions</h2>
<figure>
<img
data-src="https://www.researchgate.net/publication/339991922/figure/fig4/AS:870241110339586@1584493057180/Plot-of-different-activation-functions-a-Sigmoid-activation-function-b-Tanh.ppm"
alt="Activation Functions, from Kandel and Castelli 2020" />
<figcaption aria-hidden="true">Activation Functions, from Kandel and
Castelli 2020</figcaption>
</figure>
<ul>
<li class="fragment">Note technically, for backpropagation to work,
partial derivative must be defined.</li>
</ul>
</section>
<section id="relu-example" class="slide level2">
<h2>ReLU Example</h2>
<div class="sourceCode" id="cb5"><pre
class="sourceCode numberSource python numberLines"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>model <span class="op">=</span> nn.Sequential(Perceptron(<span class="dv">2</span>, <span class="dv">4</span>, nn.ReLU),</span>
<span id="cb5-2"><a href="#cb5-2"></a>                      Perceptron(<span class="dv">4</span>, <span class="dv">4</span>, nn.ReLU),</span>
<span id="cb5-3"><a href="#cb5-3"></a>                      Perceptron(<span class="dv">4</span>, <span class="dv">4</span>, nn.ReLU),</span>
<span id="cb5-4"><a href="#cb5-4"></a>                      Perceptron(<span class="dv">4</span>, <span class="dv">1</span>, nn.Sigmoid))</span></code></pre></div>
<div class="fragment">
<iframe width="100%" height="400px" frameborder="0" seamless="seamless" scrolling="no" src="figures/figure15.html">
</iframe>
</div>
</section></section>
<section>
<section id="recap" class="title-slide slide level1">
<h1>Recap</h1>

</section>
<section id="comprehension-check" class="slide level2">
<h2>Comprehension Check</h2>
<ol type="1">
<li class="fragment"><strong>Neural networks</strong> are basically
nested regression models with discontinuities.</li>
<li class="fragment"><strong>Loss gradients</strong> link model
parameters to the overall accuracy of the model.</li>
<li class="fragment"><strong>Backpropagation</strong> is a technique
that allows us to calculate the loss gradient of every parameter in a
network.</li>
<li class="fragment"><strong>Gradient descent</strong> is an algorithm
for updating parameters using loss gradients to improve accuracy.</li>
<li class="fragment">By “stacking” neurons and introducing
discontinuities, we can learn complex patterns in data.</li>
</ol>
</section>
<section id="next-time" class="slide level2">
<h2>Next Time</h2>
<ul>
<li class="fragment">Scaling up further: modular architectures and
mechanisms.</li>
<li class="fragment">“Pretraining” models: self-supervision and transfer
learning.</li>
<li class="fragment">Generative models: autoregressive language
generation.</li>
<li class="fragment">Architectures for sequences: recursive neural
networks (RNNs), Transformers.</li>
</ul>
</section>
<section id="excellent-resources" class="slide level2">
<h2>Excellent Resources</h2>
<ul>
<li class="fragment"><a
href="http://playground.tensorflow.org">playground.tensorflow.org</a></li>
<li class="fragment"><a
href="https://www.youtube.com/watch?v=tIeHLnjs5U8">Backpropagation
Calculus, Three Blue One Brown</a></li>
<li class="fragment"><a
href="https://www.youtube.com/watch?v=VMj-3S1tku0">Neural Networks from
Scratch, Andrej Karpathy</a></li>
</ul>
<!--docker run --rm --volume "`pwd`:/data" --user `id -u`:`id -g` pandoc/latex:2.18 -t revealjs --slide-level 2 -V theme=white --mathjax --citeproc -i -s -o presentation.html presentation.md --self-contained-->
</section></section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
