{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8552c4ae",
   "metadata": {},
   "source": [
    "# Demystifying Deep Learning Part 1 Code Notebook\n",
    "\n",
    "_Author: Dr Musashi Jacobs-Harukawa, DDSS Princeton_\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This code notebook is designed both as supplementary resource to the lecture (presentation.md in this github repository).\n",
    "\n",
    "A few things to note at the outset:\n",
    "\n",
    "- I use PyTorch. Alternatives are available (primarily Tensorflow and JAX), but a) PyTorch abstracts at a very good level for understanding what is going on, b) it a dominant framework in industry and c) it's what I know best.\n",
    "- The visualization code and other bits that I thought were less directly relevant are tucked away in the accompanying `demystifying_utils.py` file. You are welcome to inspect them if you're interested in these steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84385f3d-9eec-4dfe-93f4-896b9df1f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the accompanying functions with the following command\n",
    "!wget https://raw.githubusercontent.com/muhark/nn-tutorial/main/part1/demystifying_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcb24e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Some utilities\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Optional, Literal\n",
    "\n",
    "# Tools for creating toy datasets\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization Tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Core libraries for deep learning and numerical computing\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50b071",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression, Sort of Manually\n",
    "\n",
    "- We begin in the familiar territory of trying to fit the best linear approximation of a relationship between two variables.\n",
    "- This is something we do all the time in quantitative analysis - regression analysis.\n",
    "- But whereas we previously focus on the model specification and interpretation part, this time we are going to focus on the \"fitting\" component.\n",
    "- This exercise will help you understand a lot of the low-level operations that go on in neural networks!\n",
    "\n",
    "Let's create some made-up data from scratch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52226e16",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=0) # Reproducible random number generator\n",
    "N_         = 100\n",
    "coef_      = 3.4\n",
    "intercept_ = 1.0\n",
    "std_       = 1.3\n",
    "\n",
    "X_ = rng.uniform(low=-5, high=5, size=N_).reshape(-1, 1)\n",
    "y_ = coef_*X_ + intercept_ + rng.normal(loc=0, scale=std_, size=N_).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44110b21",
   "metadata": {},
   "source": [
    "Note that even though I record the actual coefficient for the DGP, this won't be the optimal coefficient for the sampled data.\n",
    "\n",
    "We can quickly check this by fitting a linear model (also from the `scikit-learn` library).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc49744d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# OLS Model\n",
    "lin_reg = LinearRegression()     # Instantiate linear regression model\n",
    "lin_reg.fit(X_, y_)              # Fit to data\n",
    "wOLS = lin_reg.coef_.item()      # .coef_ holds weights (betas) and\n",
    "bOLS = lin_reg.intercept_.item() # .item() returns the scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695dcf53",
   "metadata": {},
   "source": [
    "In this figure I show the data points, the \"true\" model based off of the data-generating process, and the best model for the data using the OLS method.\n",
    "\n",
    "(Feel free to check the accompanying file `demystifying_utils.py` if you want to know how the visualizations are made).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939f53a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from demystifying_utils import visualize_linear_model\n",
    "fig = visualize_linear_model(X_, y_, wOLS, bOLS)\n",
    "# fig.write_html(write_to_html='figures/figure1.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bca817",
   "metadata": {},
   "source": [
    "### How do we fit a line to some data point?\n",
    "\n",
    "- Take a guess\n",
    "- Calculate how far \n",
    "\n",
    "PyTorch offers built-in tools to conduct this process.\n",
    "\n",
    "Let's begin by building our model up from values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd63ab",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Let's convert our data to PyTorch tensors (float dtype)\n",
    "X_, y_ = torch.tensor(X_).float(), torch.tensor(y_).float()\n",
    "\n",
    "# Fit model y = wx + b: learn w and b\n",
    "# Creating tensors to store values of w and b\n",
    "# Initializations as \"empty\"\n",
    "w = torch.empty(1, 1, requires_grad=True)   # 1x1 uninitialised weight matrix\n",
    "b = torch.empty(1, 1, requires_grad=True)   # 1x1 uninitialised bias matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b252f13",
   "metadata": {},
   "source": [
    "For our first \"guess\", we can just use some random values.\n",
    "\n",
    "There are more principled approaches to this step, but these are beyond the scope of this tutorial.\n",
    "\n",
    "Instead we initialize the weights and biases with draws from the random normal distribution using ``nn.init.normal_(<param>)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e8096",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We randomize the values of these coefficients\n",
    "torch.manual_seed(0)\n",
    "with torch.no_grad():                       # Will explain this later\n",
    "    nn.init.normal_(w, mean=0, std=0.5)     # Fill with random values\n",
    "    nn.init.normal_(b, mean=0, std=0.5)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5d1c1",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig.add_trace(go.Scatter(x=[X_.min(), X_.max()],\n",
    "                         y=[(X_.min() * w + b).detach().squeeze(), (X_.max() * w + b).detach().squeeze()],\n",
    "                         mode='lines',\n",
    "                         name=f'Random Guess: {w.item():.3g}x+{b.item():.3g}',\n",
    "                         line=dict(color='green', dash='dash')))\n",
    "fig.update_layout(title='Initial (Random) Guess')\n",
    "# fig.write_html('figures/figure2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de1710",
   "metadata": {},
   "source": [
    "What does this first guess look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209d284",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f84ab1",
   "metadata": {},
   "source": [
    "How do we improve this guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df516f-d899-4225-bc59-45e1d03c3040",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "An algorithm for iteratively updating the parameters of a model to improve its fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9946ea4-7aef-4995-b1e0-b6cc247689af",
   "metadata": {},
   "source": [
    "Let's begin with the first point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1025f4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "x0 = X_[0]\n",
    "y0 = y_[0]\n",
    "\n",
    "print(x0, y0, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3823401",
   "metadata": {},
   "source": [
    "Let's generate predictions for the value of `y` for each of these two points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9d2bf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "yhat_ = x0 * w + b\n",
    "print(yhat_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca040cf2",
   "metadata": {},
   "source": [
    "How wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ede96",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "error = (y0 - yhat_)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a48d7",
   "metadata": {},
   "source": [
    "However, we need a loss function that has a minimum at zero for reasons discussed in the lecture.\n",
    "\n",
    "There are multiple options for this; in this case we square the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e88aed",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Begin by defining the loss function: squared loss \n",
    "loss = (y0 - yhat_).pow(2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5fac9",
   "metadata": {},
   "source": [
    "How does this connect to increasing the accuracy of the model?\n",
    "\n",
    "Think about the function measuring loss as a function of the weight and bias: $L(w, b)$\n",
    "\n",
    "We want to know how we should adjust the changeable parameters ($w$ and $b$) in order to reduce the size of our mistake ($L$).\n",
    "\n",
    "We can do this with a bit of calculus. The _partial derivative_ $\\frac{\\delta L}{\\delta w}$ describes how $L$ changes as a function of $w$, holding all else constant.\n",
    "\n",
    "Pytorch contains the tools to automatically calculate this, but let's do it by hand so that we make sure that we understand it.\n",
    "\n",
    "Returning to the definition of a partial derivative (https://en.wikipedia.org/wiki/Partial_derivative#Definition):\n",
    "\n",
    "$$\n",
    "\\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe185f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating a partial derivative by hand!\n",
    "h = 0.0001                           # h as some arbitrarily small value\n",
    "fx =  (y0 - (x0 *  w    + b)).pow(2) # f(x)   (squared loss)\n",
    "fxh = (y0 - (x0 * (w+h) + b)).pow(2) # f(x+h)\n",
    "dfx = (fxh-fx)/h                     # (f(x+h)-f(x))/h\n",
    "print(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6d819",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Pytorch does it for us as well\n",
    "if w.grad is not None:\n",
    "    w.grad -= w.grad            # Set gradient to 0, if any\n",
    "loss = (y0 - (x0*w + b)).pow(2) # Same loss calculation\n",
    "loss.backward()                 # Calculate dL w.r.t. all parameters (w, b)\n",
    "print(w.grad)                   # dL/dw stored on tensor w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77888e1f",
   "metadata": {},
   "source": [
    "What do we do with $\\frac{\\delta L}{\\delta w}$ and $\\frac{\\delta L}{\\delta b}$?\n",
    "\n",
    "We want to adjust the parameters in the direction of smaller loss.\n",
    "\n",
    "(Think about it--if $\\frac{\\delta L}{\\delta w}$ is positive and we increase w, then $L$ will increase!)\n",
    "\n",
    "This is easiest to see if we visualize $L(w)$ and $\\frac{\\delta L}{\\delta w}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929dd61",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# See if you can follow this code here\n",
    "wvals = []\n",
    "for wval in np.linspace(-1, 5, 100):\n",
    "    wval = torch.tensor([wval], requires_grad=True).float()\n",
    "    wval.retain_grad()\n",
    "    loss = (y0 - (x0 * wval + b)).pow(2)\n",
    "    loss.backward()\n",
    "    wvals.append([wval.item(), loss.item(), wval.grad.item()])\n",
    "\n",
    "# Make a figure from this\n",
    "temp = go.Figure(\n",
    "    data=[go.Scatter(x=[witem[0] for witem in wvals],\n",
    "                         y=[witem[1] for witem in wvals],\n",
    "                         name='L(w)',\n",
    "                         mode='markers+lines'),\n",
    "          go.Scatter(x=[witem[0] for witem in wvals],\n",
    "                         y=[witem[2] for witem in wvals],\n",
    "                         name='dL/dw',\n",
    "                         visible='legendonly',\n",
    "                         mode='markers+lines')],\n",
    "    layout=dict(title=f'Squared Loss vs Loss Gradient for w when x={x0.item():.2f} and y={y0.item():.2f}',\n",
    "                  xaxis_title='w Parameter Value',\n",
    "                  yaxis_title='Loss, dL/dw'))\n",
    "# temp.write_html('figures/figure3.html')\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec93ad",
   "metadata": {},
   "source": [
    "So how much do we adjust our parameters $w$ and $b$?\n",
    "\n",
    "Note that if we update our model to completely eliminate loss for each observation, then our model will bounce around between perfectly describing individual data points (and fail to capture some global structure).\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm where after each guess, we adjust the parameter $w$ using the following formula:\n",
    "\n",
    "$$w' = w - \\eta \\frac{dL}{dw}$$\n",
    "\n",
    "Where $\\eta$ is a parameter called the learning rate.\n",
    "\n",
    "_Learning Rate_\n",
    "\n",
    "A \"penalty\" on each update to limit overfitting on the basis of each individual point.\n",
    "\n",
    "The exact value of the learning rate is a non-trivial hyperparameter to the model, and standard practice now is to vary it during training.\n",
    "\n",
    "We will not cover this in depth, however.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ed72f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3 # Constant learning rate of 0.001\n",
    "\n",
    "# Let's see if our loss goes down!\n",
    "loss = (y0 - (x0*w + b)).pow(2) # Old loss\n",
    "print(\"Old Loss:\", loss.item())\n",
    "\n",
    "# Backprop loss\n",
    "loss.backward()\n",
    "\n",
    "# Gradient descent formula w' = w - (lr * w.grad)\n",
    "with torch.no_grad(): # Don't record this calculation\n",
    "    w -= lr * w.grad\n",
    "    b -= lr * b.grad\n",
    "\n",
    "print(\"New Loss (after update): \", ((y0 - (x0*w + b)).pow(2)).item()) # New loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b43ccf",
   "metadata": {},
   "source": [
    "Let's see how this guess compares to our previous one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db735093",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "fig.add_trace(go.Scatter(x=[X_.min(), X_.max()],\n",
    "                         y=[(X_.min() * w + b).detach().squeeze(), (X_.max() * w + b).detach().squeeze()],\n",
    "                         mode='lines',\n",
    "                         name=f'First Guess: {w.item():.3g}x+{b.item():.3g}',\n",
    "                         line=dict(color='red', dash='dash')))\n",
    "fig.update_layout(title='Guess After 1 Observation/Update, $\\eta=0.001$')\n",
    "# fig.write_html('figures/figure4.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5fc6f",
   "metadata": {},
   "source": [
    "We can repeat this for the entire dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6ff76-bc1a-4b92-92c7-124adb92cbb2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "losses = []                    # Tracking loss\n",
    "for i in tqdm(range(1, N_)):   # Skipping the observation we already have\n",
    "    w.grad.zero_; b.grad.zero_ # Reset gradients\n",
    "    x0 = X_[i]                 # Draw new samples\n",
    "    y0 = y_[i]\n",
    "    pred = (x0*w + b)          # Forward pass\n",
    "    loss = (y0 - pred).pow(2)  # Loss calculation\n",
    "    losses.append(loss.item()) # Record loss\n",
    "    loss.backward()            # Backward pass \n",
    "    with torch.no_grad():      # Manual gradient descent\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0effd-b129-499a-80be-964cbefe6a51",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Final figure\n",
    "fig.add_trace(go.Scatter(x=[X_.min(), X_.max()],\n",
    "                         y=[(X_.min() * w + b).detach().squeeze(), (X_.max() * w + b).detach().squeeze()],\n",
    "                         mode='lines',\n",
    "                         name=f'Final Guess: {w.item():.3g}x+{b.item():.3g}',\n",
    "                         line=dict(dash='dash')))\n",
    "fig.update_layout(title='Guess After 1 Epoch')\n",
    "# fig.write_html('figures/figure5.html')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d93c0b-bc09-4869-a460-220f6e850e5a",
   "metadata": {},
   "source": [
    "### Challenge 1: Batch Gradient Descent\n",
    "\n",
    "Stochastic gradient descent does one observation at a time.\n",
    "\n",
    "How would you implement batch gradient descent, which uses the full dataset each time?\n",
    "\n",
    "Enter code in the below box. The correct answers are in the boxes below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a64fe-fb1f-4014-882c-6f4856869236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ee416-d56f-4f8a-83e4-8cef40c44bba",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971ee22-e1d7-4618-8191-215f09f4a20e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w.grad.zero_; b.grad.zero_       # Reset gradients\n",
    "pred = (X_*w+b)                  # Forward pass\n",
    "loss = (y_-pred).pow(2).mean()   # Means squared error loss\n",
    "losses.append(loss.item())       # Record loss\n",
    "loss.backward()                  # Backward pass \n",
    "with torch.no_grad():            # Manual gradient descent\n",
    "    w -= lr * w.grad\n",
    "    b -= lr * b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26c7ae-0976-49a0-9272-313823cb9bc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 30 epochs batch gd\n",
    "epochs = 30\n",
    "losses = []\n",
    "for _ in tqdm(range(epochs)):\n",
    "    w.grad.zero_; b.grad.zero_       # Reset gradients\n",
    "    pred = (X_*w+b)                  # Forward pass\n",
    "    loss = (y_-pred).pow(2).mean()   # Means squared error loss\n",
    "    losses.append(loss.item())       # Record loss\n",
    "    loss.backward()                  # Backward pass \n",
    "    with torch.no_grad():            # Manual gradient descent\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e70d4f",
   "metadata": {},
   "source": [
    "## Part 2: Getting Harder\n",
    "\n",
    "So far we've seen a slower and harder way to do regression.\n",
    "\n",
    "Let's consider a problem that would be difficult (impossible) with a linear regression approach.\n",
    "\n",
    "It's easiest to visualize it first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae92be",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from demystifying_utils import generate_2d_data\n",
    "\n",
    "X, y = generate_2d_data('Moons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ef42f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split to simulate \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Visualize dataset, train/test split\n",
    "# sorry--didn't have time to move it yet\n",
    "fig = go.Figure()\n",
    "fig.add_trace( # Visualize train data\n",
    "    go.Scatter(x=X_train[:, 0],\n",
    "               y=X_train[:, 1], \n",
    "                mode='markers',\n",
    "                name='Train Set',\n",
    "                hovertemplate=\"%{x:.3g}, %{y:.3g}\",\n",
    "                marker=dict(size=3, color=y_train, colorscale='bluered_r'),\n",
    "                text=['Class 1' if y==1 else 'Class 0' for y in y_train]))\n",
    "fig.add_trace( # Visualize test data\n",
    "    go.Scatter(x=X_test[:, 0],\n",
    "               y=X_test[:, 1],\n",
    "               mode='markers',\n",
    "               name='Test Set',\n",
    "               hovertemplate=\"%{x:.3g}, %{y:.3g}\",\n",
    "               marker=dict(size=7, color=y_test, colorscale='bluered_r'),\n",
    "               text=['Class 1' if y==1 else 'Class 0' for y in y_train]))\n",
    "fig.update_layout(title='Classification Problem in Two Dimensions',\n",
    "                  xaxis_title='X1',\n",
    "                  yaxis_title='X2',\n",
    "                  showlegend=True)\n",
    "# fig.write_html('figures/figure6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f08c3",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from demystifying_utils import ModelVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de31ff",
   "metadata": {},
   "source": [
    "### Using Classes and Modules\n",
    "\n",
    "In the previous section, we used vectors (tensors) to hold all of our parameters and updated them against data.\n",
    "\n",
    "In this step we're going to change from our mathematician to our engineer hat.\n",
    "\n",
    "Let's create reusable blueprints to hold our models (both the structure and the parameters) and then populate them with data.\n",
    "\n",
    "Take time to look at this code line-by-line (and the comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeef7c4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class LinearNeuron(nn.Module):         # It inherits methods from nn.Module\n",
    "    def __init__(self,\n",
    "                 features_in: int=1,     # Number of features in \n",
    "                 features_out: int=1,    # Number of features out\n",
    "                 bias: bool=True):       # Add bias parameter?\n",
    "        super(LinearNeuron,              # This basically adds methods from\n",
    "              self).__init__()           # nn.Module to this class.\n",
    "        self.weights = nn.Parameter(     # Need to define as parameter so\n",
    "            torch.empty(                 # optimizer knows to optimize it.\n",
    "                features_in,\n",
    "                features_out))\n",
    "        nn.init.normal_(self.weights)    # Initialize to some sane values\n",
    "        self.bias = nn.Parameter(        # Some models do not need a bias\n",
    "            torch.empty(1, features_out) # parameter, this is one way\n",
    "            ) if bias else 0             # to implement that.\n",
    "        if bias:\n",
    "            nn.init.normal_(self.bias)\n",
    "    \n",
    "    def forward(self, inputs):           # Every nn.Module needs a forward func\n",
    "        preds = torch.matmul(            # Define forward pass \n",
    "            inputs, self.weights         # W∙X\n",
    "            ) + self.bias                # + b\n",
    "        return preds                     # return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58e2d4",
   "metadata": {},
   "source": [
    "We create an instance of this blueprint and call it `slnn`.\n",
    "\n",
    "_Comprehension Check_: why do we set `features_in=2` below?\n",
    "\n",
    "\n",
    "_Answer_: because we have two input variables, X1 and X2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eadf28",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Construct network\n",
    "slnn = LinearNeuron(features_in=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193282b9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# %% Visualize the uninitialised model and its predictions\n",
    "mv = ModelVisualization(slnn, X_test, y_test, h=0.1)\n",
    "mv.fig.update_layout(title=\"Predictions of Uninitialised Model\")\n",
    "mv.fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bf077",
   "metadata": {},
   "source": [
    "### Training a Module\n",
    "\n",
    "This time, instead of using one observation at a time, we'll work with _batches_ of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92fe5b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Again, let's convert our data to tensors\n",
    "X_train, y_train = (torch.tensor(X_train, dtype=torch.float32),\n",
    "                    torch.tensor(y_train, dtype=torch.float32\n",
    "                                ).reshape(-1, 1)) # Needs to be 2D\n",
    "\n",
    "# Use first 8 observations as first batch\n",
    "inputs = X_train[:8, :]\n",
    "labels = y_train[:8, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af533fcb",
   "metadata": {},
   "source": [
    "Instead of manually doing the gradient updates, we use an optimizer from `torch.optim`.\n",
    "\n",
    "In this case I am using a Stochastic Gradient Descent (`SGD`) optimizer, which functions the same way as the manual updates we were doing above.\n",
    "\n",
    "In most modern applications, we would use a better optimizer (namely `Adam` or its variants).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6deda",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optim = torch.optim.SGD(slnn.parameters(), lr=5e-4)\n",
    "\n",
    "# Reset the gradients\n",
    "optim.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157306d",
   "metadata": {},
   "source": [
    "For the forward and backward pass, we can also use a built-in function for the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be262148",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass: Data -> Predictions\n",
    "preds = slnn(inputs) # We don't actually need to use the `.forward` function\n",
    "\n",
    "# Loss calculation\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(preds, labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb902c",
   "metadata": {},
   "source": [
    "Now we can use the optimizer to update the weights with the `.step()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4997f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "print(f\"Weights: {slnn.weights}\",\n",
    "      f\"Weights: {slnn.bias}\",\n",
    "      sep='\\n')\n",
    "print(\"=\"*12)\n",
    "optim.step()\n",
    "print(\"PARAMETER UPDATE\")\n",
    "print(\"=\"*12)\n",
    "print(f\"Weights: {slnn.weights}\",\n",
    "      f\"Weights: {slnn.bias}\",\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd20f6",
   "metadata": {},
   "source": [
    "### An Aside on Dataloaders\n",
    "\n",
    "Note that above I manually selected the first 8 rows of the dataset to train the model.\n",
    "\n",
    "This is an inefficient/inflexible approach.\n",
    "\n",
    "Pytorch provides powerful tools for feeding data to your model.\n",
    "\n",
    "It's a bit of a distraction to go into them for now, but we can come back at the end if there's time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd9f0a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X: torch.tensor, y: torch.tensor):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    \n",
    "    def __len__(self): # Required method 1; must return length of data as int\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx: int): # Required method 2; how to grab data\n",
    "        return self.features[idx, :], self.labels[idx, :]\n",
    "\n",
    "\n",
    "dataset = SimpleDataset(X_train, y_train) # Instantiate dataset object\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True) # Wrap with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3570b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "slnn = LinearNeuron(2)\n",
    "optim = torch.optim.SGD(slnn.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "mv = ModelVisualization(slnn, X_test, y_test, h=0.1)\n",
    "# mv.fig.write_html('figures/figure7.html')\n",
    "mv.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cffdff",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "# Single epoch of training\n",
    "    for bn, (inputs, labels) in enumerate(dataloader):\n",
    "        # Reset gradients\n",
    "        optim.zero_grad()\n",
    "        # Forward\n",
    "        preds = slnn(inputs)\n",
    "        # Loss\n",
    "        loss = loss_fn(preds, labels)\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b1240",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mv = ModelVisualization(slnn, X_test, y_test, h=0.025)\n",
    "mv.fig.update_layout(title=\"Predictions After 50 Epochs\")\n",
    "# mv.fig.write_html('figures/figure8.html')\n",
    "mv.fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d68aa",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "After 50 epochs, the model is doing roughly what we want. But there are a few improvements we might want:\n",
    "\n",
    "- Better fit to response surface\n",
    "- Classification instead of regression\n",
    "\n",
    "Let's tackle the fit problem first (because the classification step is easy)\n",
    "\n",
    "### Layering/Depth\n",
    "\n",
    "Stacking and widening neural networks.\n",
    "\n",
    "Let's revisit our original neural network. It has two inputs and a single output. We can represent it as the following diagram:\n",
    "\n",
    "<!-- ![Simple Neural Network](./figures/nn2-1.svg) -->\n",
    "\n",
    "In this diagram:\n",
    "\n",
    "- nodes are data points\n",
    "- edges are model weights\n",
    "\n",
    "Therefore this diagram represents a regression model with X values, two weights (biases are omitted for ease of presentation), and a single output y.\n",
    "\n",
    "But what if we create a wider network like the following?\n",
    "\n",
    "<!-- ![Simple Neural Network](./figures/nn2-4.svg) -->\n",
    "\n",
    "What is going on here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c1b57",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "widenn = LinearNeuron(features_in=2, features_out=4)\n",
    "widenn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab218d0",
   "metadata": {},
   "source": [
    "What does four outputs mean?\n",
    "\n",
    "You can think of this as training four separate regression models that take the same inputs, and output four separate values.\n",
    "\n",
    "How do we turn four predictions back into one?\n",
    "\n",
    "We can use a new network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ccd1f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "widenn2 = LinearNeuron(features_in=4, features_out=1)\n",
    "\n",
    "hidden_layer = widenn(inputs)\n",
    "preds = widenn2(hidden_layer)\n",
    "print(inputs.shape, hidden_layer.shape, preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6af12a",
   "metadata": {},
   "source": [
    "Combining the two networks, we get something that looks like this:\n",
    "\n",
    "<!-- ![Deep Neural Network](./part1/figures/nn2-4-1.svg) -->\n",
    "\n",
    "Can we train our wide network?\n",
    "\n",
    "Pytorch offers a convenient method for stacking networks:  `nn.Sequential`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0114c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sdnn = nn.Sequential(widenn, widenn2)\n",
    "sdnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcd74c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mvdnn = ModelVisualization(sdnn, X_test, y_test, h=0.1)\n",
    "mvdnn.fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f6ce8f",
   "metadata": {},
   "source": [
    "Before we fit this model to the data, any guesses about how the prediction surface might change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f27ba3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We can calculate the average mistake before and after\n",
    "nn.functional.mse_loss(                          # mse_loss function\n",
    "    sdnn(torch.tensor(X_test).float()),          # Prediction\n",
    "    torch.tensor(y_test).float().reshape(-1, 1)) # Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99e005",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 50 training epochs\n",
    "optim = torch.optim.SGD(sdnn.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for bn, (inputs, labels) in enumerate(dataloader):\n",
    "        optim.zero_grad()\n",
    "        preds = sdnn(inputs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "# Test loss\n",
    "nn.functional.mse_loss(                          # mse_loss function\n",
    "    sdnn(torch.tensor(X_test).float()),          # Prediction\n",
    "    torch.tensor(y_test).float().reshape(-1, 1)) # Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2cde3",
   "metadata": {},
   "source": [
    "But--prediction surface is still flat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ad97c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mvdnn = ModelVisualization(sdnn, X_test, y_test, h=0.025)\n",
    "mvdnn.fig.update_layout(title=\"Predictions After 50 Epochs\")\n",
    "# mvdnn.fig.write_html('figures/figure9.html')\n",
    "mvdnn.fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508c3eb",
   "metadata": {},
   "source": [
    "It turns out that any linear combination of linear models reduces to a linear model. (A proof is beyond the scope of this workshop and also I haven't sat down to work it out).\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "The other key component to neural networks is the activation function.\n",
    "\n",
    "You've probably already come across activation functions before--logistic regression!\n",
    "\n",
    "Logistic regressions can be thought of as a sigmoid transformation on the output of a linear model.\n",
    "\n",
    "Let's define a new neural network template that can take activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebe574",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self,\n",
    "                 features_in: int=1,\n",
    "                 features_out: int=1,\n",
    "                 activation: nn.modules.activation=nn.Sigmoid,\n",
    "                 bias: bool=True):\n",
    "        # This is same as before\n",
    "        super(Perceptron,\n",
    "              self).__init__()\n",
    "        self.weights = nn.Parameter( torch.empty(features_in, features_out))\n",
    "        nn.init.normal_(self.weights)\n",
    "        self.bias = nn.Parameter(torch.empty(1, features_out)) if bias else 0\n",
    "        if bias:\n",
    "            nn.init.normal_(self.bias)\n",
    "        # This is new\n",
    "        self.activation = activation()\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        preds = torch.matmul(inputs, self.weights) + self.bias\n",
    "        output = self.activation(preds)  # Apply activation function\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d43dab",
   "metadata": {},
   "source": [
    "Aside: what _is_ the Sigmoid function?\n",
    "\n",
    "Defined as $Sigmoid(x) = \\frac{1}{1+exp(-x)}$.\n",
    "\n",
    "We can visualize it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430471d-f9a3-4d34-ab4e-fd568a87dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=torch.linspace(-5, 5, 100),\n",
    "        y=torch.sigmoid(torch.linspace(-5, 5, 100)),\n",
    "        labels = {'x': 'x', 'y': 'σ(x)'},\n",
    "        title = 'Sigmoid function')#.write_html('figures/sigmoid.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7001ed",
   "metadata": {},
   "source": [
    "The key takeaways:\n",
    "\n",
    "- Symmetric over input 0\n",
    "- 0 maps to 0.5\n",
    "- Outputs are bounded between 0 and 1\n",
    "- \"Saturates\" as input magnitudes increase\n",
    "\n",
    "How do we use this for binary classification?\n",
    "\n",
    "Let's train up a logistic classifier.\n",
    "\n",
    "At this point we'll be doing a lot of training, so let's wrap our training in a function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a3572",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Coding exercise - provide type hints for this function):\n",
    "def train_n_epochs(model, dataloader, epochs, optim, loss_fn, lr=1e-3, verbose=True):\n",
    "    optim = optim(model.parameters(), lr=lr)\n",
    "    loss_fn = loss_fn()\n",
    "    for epoch in tqdm(range(epochs), disable=~verbose):\n",
    "        for inputs, labels in dataloader:\n",
    "            optim.zero_grad()\n",
    "            preds = model(inputs)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    return model\n",
    "\n",
    "def eval_model(model, x_test, y_test, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        loss = loss_fn(\n",
    "            model(torch.tensor(X_test).float()),\n",
    "            torch.tensor(y_test).float().reshape(-1, 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cb788",
   "metadata": {},
   "source": [
    "Let's instantiate, train and evaluate our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73f4c5-f46e-4665-b23b-cae40cc9d286",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "model = Perceptron(2, 1, nn.Sigmoid) # 2 inputs, 1 output, Sigmoid activation\n",
    "loss_fn = nn.functional.binary_cross_entropy_with_logits # We use binary cross entropy for binary outcomes\n",
    "print(eval_model(model, X_test, y_test, loss_fn)) # Eval starting point\n",
    "\n",
    "# Train\n",
    "train_n_epochs(model, dataloader, 50, torch.optim.SGD, nn.BCELoss, lr=0.03)\n",
    "\n",
    "# Eval\n",
    "print(eval_model(model, X_test, y_test, loss_fn)) # Eval completion\n",
    "\n",
    "# Visualize predictions\n",
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=\"Simple Logistic Regression After 50 Epochs\")#.write_html('figures/figure10.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af26b08",
   "metadata": {},
   "source": [
    "We still have a \"flat\" decision boundary, however. How can we improve this?\n",
    "\n",
    "What happens if we stack two logistic models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb2648",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 2, nn.Sigmoid),\n",
    "                       Perceptron(2, 1, nn.Sigmoid))\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "train_n_epochs(model, dataloader, 1000, torch.optim.SGD, nn.BCELoss, lr=0.03)\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "ModelVisualization(model, X_test, y_test, h=0.02).fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fefc8b",
   "metadata": {},
   "source": [
    "What if we widen the middle layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0c219",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 4, nn.Sigmoid),\n",
    "                      Perceptron(4, 1, nn.Sigmoid))\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "train_n_epochs(model, dataloader, 1000, torch.optim.SGD, nn.BCELoss, lr=0.03)\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "# ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=\"2-4-1 Logistic Network after 1000 Epochs\").write_html('figures/figure11.html')\n",
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=\"2-4-1 Logistic Network after 1000 Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0503b",
   "metadata": {},
   "source": [
    "Wider and deeper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07e21b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 8, nn.Sigmoid),\n",
    "                      Perceptron(8, 8, nn.Sigmoid),\n",
    "                      Perceptron(8, 1, nn.Sigmoid))\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "train_n_epochs(model, dataloader, 1000, torch.optim.SGD, nn.BCELoss, lr=0.03)\n",
    "print(eval_model(model, X_test, y_test, loss_fn))\n",
    "# ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=\"2-8-8-1 Logistic Network after 1000 Epochs\").write_html('figures/figure12.html')\n",
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=\"2-8-8-1 Logistic Network after 1000 Epochs\")#.write_html('figures/figure12.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ae355",
   "metadata": {},
   "source": [
    "### Training to completion\n",
    "\n",
    "You might notice that the performance is a bit sensitive with respect to the random initialization.\n",
    "\n",
    "Supposing that we took three splits of the data: train, eval, test\n",
    "\n",
    "We could train on train, use eval to decide when it's done, and then test on test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a93a6",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 4, nn.Sigmoid),\n",
    "                      Perceptron(4, 1, nn.Sigmoid))\n",
    "total_epochs=0\n",
    "loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "while loss > 0.52:\n",
    "    print(loss)\n",
    "    train_n_epochs(model, dataloader, 100, torch.optim.SGD, nn.BCELoss, lr=0.03, verbose=False)\n",
    "    loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "    total_epochs += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda95c4-ba79-4065-9ac1-5466a63c296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=f\"2-4-1 Logistic Network after {total_epochs} Epochs\")#.write_html('figures/figure13.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f97f40",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 4, nn.Sigmoid),\n",
    "                      Perceptron(4, 4, nn.Sigmoid),\n",
    "                      Perceptron(4, 4, nn.Sigmoid),\n",
    "                      Perceptron(4, 1, nn.Sigmoid))\n",
    "total_epochs=0\n",
    "loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "while loss > 0.52:\n",
    "    print(loss)\n",
    "    train_n_epochs(model, dataloader, 100, torch.optim.SGD, nn.BCELoss, lr=0.03)\n",
    "    loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "    total_epochs += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc7d00",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=f\"2-4-4-4-1 Logistic Network after {total_epochs} Epochs\")#.write_html('figures/figure14.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411d524",
   "metadata": {},
   "source": [
    "On a final note, other activation functions exist:\n",
    "\n",
    "![Activation Functions, from Kandel and Castelli 2020](https://www.researchgate.net/publication/339991922/figure/fig4/AS:870241110339586@1584493057180/Plot-of-different-activation-functions-a-Sigmoid-activation-function-b-Tanh.ppm)\n",
    "<!-- ![Logistic Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1024px-Logistic-curve.svg.png) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc611d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(Perceptron(2, 4, nn.ReLU),\n",
    "                      Perceptron(4, 4, nn.ReLU),\n",
    "                      Perceptron(4, 4, nn.ReLU),\n",
    "                      Perceptron(4, 1, nn.Sigmoid))\n",
    "total_epochs=0\n",
    "loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "while loss > 0.52:\n",
    "    print(loss)\n",
    "    train_n_epochs(model, dataloader, 100, torch.optim.SGD, nn.BCELoss, lr=0.03, verbose=False)\n",
    "    loss = eval_model(model, X_test, y_test, loss_fn)\n",
    "    total_epochs += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efb483",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ModelVisualization(model, X_test, y_test, h=0.02).fig.update_layout(title=f\"ReLU Network after {total_epochs} Epochs\")#.write_html('figures/figure15.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
